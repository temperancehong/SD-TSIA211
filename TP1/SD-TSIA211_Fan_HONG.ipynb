{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5401340d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dependencies\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "44503857",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading data\n",
    "data_matrix_train, COP_train, data_matrix_test, COP_test, names = np.load('data_center_data_matrix.npy', allow_pickle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56aa4298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(722, 892)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_matrix_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fbfdccc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(722, 4)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COP_train.shape # KPI, in total 4 KPIs, and there are 722 measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc9964cb",
   "metadata": {},
   "source": [
    "# 3. Least Squares\n",
    "## Question 3.1\n",
    "\n",
    "If $Aw = b$ then we have $y(t) = \\tilde x(t)^Tw_1+w_0-y(t)\\tilde x(t)^Tw_2$. Therefore we have:\n",
    "\n",
    "$$y(t) = \\frac{w_1^T\\tilde x(t)+w_0}{\\tilde x(t)^Tw_2+1}$$\n",
    "\n",
    "## Question 3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e9052c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def standardize_data(data):\n",
    "    mean = np.mean(data, axis=0)\n",
    "    M = data - mean\n",
    "    std = np.std(M, axis=0)\n",
    "    M = M/std\n",
    "    return M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "184fc100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# standardize the matrix\n",
    "matrix_mean = np.mean(data_matrix_train, axis=0)\n",
    "M = data_matrix_train - matrix_mean\n",
    "matrix_std = np.std(M, axis=0)\n",
    "M = M / matrix_std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e5c2e9a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(892, 722)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(-(M.T * COP_train[:,3])).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bf8a7b87",
   "metadata": {},
   "outputs": [],
   "source": [
    "# constructing A and b for solving min_w ||A w - b||_2**2\n",
    "## point wise multiplication broadcast\n",
    "# A of shape (n, 2m+1)\n",
    "A = np.hstack([M, np.ones((M.shape[0],1)), -(M.T * COP_train[:,3]).T]) # -(M.T * COP_train[:,3]).T shape (722, 892)\n",
    "b = COP_train[:,3] # KPI number 3, shape (n,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0964e96a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# solving the linear question, getting the w vector\n",
    "# here we have m = d in the definition\n",
    "w = np.linalg.lstsq(A,b, rcond=None)[0] # of shape 2d+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b21086d",
   "metadata": {},
   "source": [
    "## Question 3.3\n",
    "\n",
    "Evaluate the quality of the solution with MSE and R2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "149f48d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constructing matrices for the test set\n",
    "M_test = (data_matrix_test - matrix_mean) / matrix_std # use training data mean for test, we cannot suppose the results for test\n",
    "A_test = np.hstack([M_test, np.ones((M_test.shape[0],1)), -(M_test.T * COP_test[:,3]).T])\n",
    "b_test = COP_test[:,3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d0b7980e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(780.8984793523456, -40.87585582906847)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_predict = A_test @ w\n",
    "n_test = b_test.shape[0]\n",
    "mse_1 = 1/n_test * np.sum((b_test - b_predict)**2)\n",
    "R2_1 = 1- np.sum((b_test - b_predict)**2)/np.sum((b_test - b_test.mean())**2)\n",
    "mse_1, R2_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "48596cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_score(A, w, b):\n",
    "    residuals = A@w-b\n",
    "    return (0.5 * np.linalg.norm(residuals)**2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1194a3e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "140952.17552309835"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_score(A_test, w, b_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c310c9e",
   "metadata": {},
   "source": [
    "## Question 3.4\n",
    "\n",
    "We will use the result of the following question with the gradient, and we set the gradient to 0, i.e.: $A^T(Aw-b) + \\lambda w = 0$. Therefore solving the problem becomes making $A^TAw + \\lambda w = A^Tb$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5d505401",
   "metadata": {},
   "outputs": [],
   "source": [
    "lambda_ridge = 100\n",
    "w2 = np.linalg.lstsq(A.T @ A + lambda_ridge * np.identity((A.T @ A).shape[0]),(A.T @ b), rcond=None)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "8fccf3b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(301.0548280945075, -15.14413257455234)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b_predict2 = A_test @ w2\n",
    "n_test = b_test.shape[0]\n",
    "mse_2 = 1/n_test * np.sum((b_test - b_predict2)**2)\n",
    "R2_2 = 1- np.sum((b_test - b_predict2)**2)/np.sum((b_test - b_test.mean())**2)\n",
    "mse_2, R2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b9709d5",
   "metadata": {},
   "source": [
    "The test MSE is about half of that in the unregularized problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aaee164",
   "metadata": {},
   "source": [
    "## Question 3.5\n",
    "\n",
    "The gradient of $w\\mapsto \\frac{1}{2}||Aw-b||^2$ is $ A^T(Ax-b)$, and the gradient of $\\frac{\\lambda}{2}||w||^2$ is $\\lambda w$. Therefore the gradient of the function is \n",
    "\n",
    "$$\\nabla f_1(w) :\\; w\\mapsto A^T(Aw-b) + \\lambda w$$\n",
    "\n",
    "To determine the convexity of a function, we have to look into its Hessian matrix.\n",
    "\n",
    "The Hessian of function $f_1$ is $H(f_1) = A^TA + \\lambda I$ which is positive semi definitive. Therefore function $f_1$ is convex."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1b78a2",
   "metadata": {},
   "source": [
    "## Question 3.6\n",
    "\n",
    "For the gradient descent, we use the process $$x_{k+1} = x_k - \\gamma_k \\nabla f_1(x_k)$$. \n",
    "\n",
    "where $\\gamma_k$ is the learning step and here we set it as a constant. We randomly initialize $w$, and the target is to get the optimal $w$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1a9322bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(722, 1)"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A @ w0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2e7d8995",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1785, 722)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A.T.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "1fe7dfcb",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1785,)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(A.T @ A @ w ).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0b2d1ad0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1785,)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1a23c757",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.76491653,  8.8438464 ,  3.76089874,  3.71666913,  3.58255932,\n",
       "        3.306774  ,  3.30792173,  6.83331686,  3.13708744,  3.33057503,\n",
       "        3.22320998,  8.12098306,  7.754193  ,  3.29406059,  3.35823478,\n",
       "        3.52437946,  2.80497745,  4.77146998,  2.73655381,  3.91922239,\n",
       "        4.03605079,  4.0651159 , 11.10411331,  3.97340026, 11.87813909,\n",
       "        3.94730954, 10.51994604,  3.84016567,  4.35510147,  4.33740932,\n",
       "        4.26665541,  4.36421565,  4.37908818, 11.67084938,  4.65837061,\n",
       "       10.577855  ,  4.32085972, 11.51069335,  4.28548163,  4.74556538,\n",
       "        4.08024852,  3.26302011,  2.95272548,  9.36877402,  3.96903821,\n",
       "        4.06816979,  4.1166304 ,  4.26764676,  4.06088352, 10.04707594,\n",
       "        4.10628677,  9.57462636,  3.72404833,  3.92685151,  9.62867667,\n",
       "        3.81973149,  4.08658883,  4.11987818,  9.82520075,  9.82459451,\n",
       "        4.21617925,  8.84144874,  4.66087375,  3.84240524,  7.86076559,\n",
       "        8.62641723,  3.64344871,  3.81441033,  3.63171803, 10.474108  ,\n",
       "        4.19258832, 10.09871133,  4.41546146,  4.18988927,  4.21883346,\n",
       "        4.26181568,  3.88473333,  4.28809457,  4.16206583,  5.99370326,\n",
       "        3.59998635,  4.16911012,  4.05677467,  4.24665661,  4.02904539,\n",
       "        3.89458892,  3.22791058,  2.65648703,  3.92172591,  3.15316294,\n",
       "        3.15316294,  3.15316294,  3.15316294,  3.15316294,  3.15316294,\n",
       "        3.15316294,  3.15316294,  3.15316294,  3.15316294,  3.15316294,\n",
       "        3.15316294,  3.15316294,  3.15316294,  3.15316294,  3.15316294,\n",
       "        3.15316294,  3.15316294,  3.15316294,  3.15316294,  3.15316294,\n",
       "        3.15316294,  3.15316294,  3.15316294,  3.15316294,  3.15316294,\n",
       "        3.15316294,  3.15316294,  3.15316294,  3.15316294,  3.15316294,\n",
       "        3.15316294,  3.15316294,  3.15316294,  3.15316294,  3.15316294,\n",
       "        3.15316294,  3.15316294,  3.15316294,  3.15316294,  3.15316294,\n",
       "        3.15316294,  3.15316294,  3.15316294,  3.15316294,  3.15316294,\n",
       "        3.15316294,  3.15316294,  3.15316294,  3.15316294,  3.15316294,\n",
       "        3.15316294,  3.15316294,  3.15316294,  3.15316294,  3.15316294,\n",
       "        4.2872239 ,  3.90123145,  4.10799606,  3.2661538 ,  3.68801835,\n",
       "        4.13485495,  4.35036292,  4.41724113,  4.36083696,  4.36083696,\n",
       "        4.36083696,  4.36083696,  4.36083696,  4.36083696,  4.36083696,\n",
       "        4.36083696,  4.36083696,  4.36083696,  4.36083696,  4.36083696,\n",
       "        4.36083696,  4.36083696,  4.36083696,  4.36083696,  4.36083696,\n",
       "        4.36083696,  4.36083696,  4.36083696,  4.36083696,  4.36083696,\n",
       "        4.36083696,  4.36083696,  4.36083696,  4.36083696,  4.36083696,\n",
       "        4.36083696,  4.36083696,  4.36083696,  4.36083696,  4.36083696,\n",
       "        2.40010367,  3.53067771,  4.33960694,  2.72469139,  3.58707232,\n",
       "        4.12832362,  2.73948053,  2.47348406,  2.42561553,  2.42561553,\n",
       "        2.42561553,  2.42561553,  2.42561553,  2.42561553,  2.42561553,\n",
       "        2.42561553,  2.42561553,  2.42561553,  2.42561553,  2.42561553,\n",
       "        2.42561553,  2.42561553,  2.42561553,  2.42561553,  2.49705151,\n",
       "        2.49705151,  2.49705151,  2.49705151,  2.49705151,  2.49705151,\n",
       "        2.49705151,  2.49705151,  2.49705151,  2.49705151,  2.49705151,\n",
       "        2.49705151,  2.49705151,  2.49705151,  2.49705151,  2.49705151,\n",
       "        2.49705151,  1.61200589,  2.40008858,  2.37603972,  2.28546463,\n",
       "        2.05403048,  2.58101782,  2.74582582,  2.94304048,  2.89799049,\n",
       "        2.7989632 ,  2.7989632 ,  2.7989632 ,  2.7989632 ,  2.7989632 ,\n",
       "        2.7989632 ,  2.7989632 ,  2.7989632 ,  2.7989632 ,  2.7989632 ,\n",
       "        2.7989632 ,  2.7989632 ,  2.7989632 ,  2.7989632 ,  2.7989632 ,\n",
       "        2.7989632 ,  2.7989632 ,  2.7989632 ,  2.54128659,  2.61437494,\n",
       "        2.56231787,  3.44713741,  3.91734194,  2.81731949,  3.85498571,\n",
       "        2.51861554,  2.40002758,  2.29553157,  2.01724398,  2.37870014,\n",
       "        2.51135603,  2.55137788,  2.61533848,  2.33743867,  2.41313828,\n",
       "        2.33708163,  2.24848276,  4.18006625,  2.93127486,  2.41522676,\n",
       "        2.98855614,  3.60483615,  2.3074574 ,  2.34100868,  2.30610647,\n",
       "        2.36898108,  2.46816033,  2.50306471,  2.60144438,  4.5353619 ,\n",
       "        2.75905103,  3.60661396,  4.58746613,  4.72446925,  4.45379469,\n",
       "       11.19872372, 10.84054133, 10.79772755, 10.85655435,  4.2722049 ,\n",
       "        4.6735171 ,  4.58102371,  4.47502949,  2.70942942,  2.86272208,\n",
       "        4.23741857,  4.67015891, 11.5023189 , 11.15039333,  4.36589903,\n",
       "        4.43031404, 11.43654734,  2.85000405,  4.51164391,  2.77351212,\n",
       "        2.84873612,  2.64410544,  4.4971383 , 11.19964716, 10.61595788,\n",
       "        4.35111347,  4.35169162, 11.16012246,  4.58197977,  2.89079627,\n",
       "        2.81292718,  2.73790763,  4.17584483,  4.38056672,  2.91726134,\n",
       "       10.60907019,  4.73082454,  4.33023475, 10.28137715,  4.45983172,\n",
       "        2.87872098,  4.35565945,  4.22391516,  4.17402689,  4.53365671,\n",
       "        3.04317759,  4.71673284,  4.50919016,  4.50899202, 10.13789698,\n",
       "       11.77210585,  2.97500768,  2.92815727,  4.05710922,  4.1737626 ,\n",
       "        2.95604591,  3.69351677,  9.52842921,  3.22886636,  3.46902353,\n",
       "       11.40299715,  4.72744347,  4.51014555,  4.73106132,  3.41102829,\n",
       "        4.45153567,  4.77373489,  4.73600933,  3.82835589,  4.69897525,\n",
       "        4.31455882,  4.48670305,  4.43251523,  4.58476113,  3.02622195,\n",
       "        4.1121305 ,  3.67172737,  4.22407199,  2.73535216,  4.33715661,\n",
       "        2.901215  ,  4.56745787,  4.60211996,  2.83884677,  4.46939653,\n",
       "        4.44231505,  5.21591691,  2.72688014,  2.66213096,  2.524128  ,\n",
       "        2.52969653,  2.54109004,  2.65007328,  4.42006785,  3.66027139,\n",
       "        3.26960029,  4.50667942,  4.19881862,  3.91511128,  2.79500243,\n",
       "        1.70898773,  2.39182403,  2.33323087,  2.40273542,  2.62050359,\n",
       "        2.23205772,  2.79596142,  2.98070845,  2.96921313,  4.39164178,\n",
       "        4.4964768 ,  2.70180918,  0.        ,  3.6499539 ,  2.05188021,\n",
       "        2.05188021,  2.05188021,  2.05188021,  2.05188021,  2.05188021,\n",
       "        2.05188021,  2.05188021,  2.05188021,  2.05188021,  2.05188021,\n",
       "        2.05188021,  2.05188021,  2.36234586,  2.36234586,  1.6411988 ,\n",
       "        4.33529688,  3.44462455,  2.55597482,  2.87966742,  3.52616462,\n",
       "        2.57004041,  2.24819717,  1.59471269,  1.82369814,  1.80168675,\n",
       "        2.85909601,  2.53195708,  2.97425998,  3.6102343 ,  4.2291795 ,\n",
       "        2.9788439 ,  3.09831168,  4.65471476,  2.36352697,  2.33709761,\n",
       "        2.26672578,  2.2416692 ,  2.50267047,  1.98260608,  4.1995797 ,\n",
       "        2.55590741,  2.02017024,  2.63795561,  4.12897932,  2.18823263,\n",
       "        1.70830293,  2.14433484,  1.67132565,  2.38344733,  2.40803602,\n",
       "        2.52922595,  2.54909867,  2.34911942,  4.24647564,  2.69549587,\n",
       "        3.26076644,  2.33956945,  2.30833076,  2.20727027,  2.18610144,\n",
       "        2.33080375,  3.74611686,  3.14582102,  2.67185715,  2.52100309,\n",
       "        2.56095783,  3.34339681,  3.29924603,  2.60928889,  1.80407549,\n",
       "        2.26199403,  2.21888423,  2.35135852,  2.37416689,  2.18791826,\n",
       "        1.82361499,  2.36761084,  3.45867813,  3.18821511,  3.57024223,\n",
       "        4.46299982,  2.53732438,  2.5567692 ,  2.53539726,  2.59434568,\n",
       "        2.21387197,  3.68354956,  4.46157452,  4.68966293,  4.32178186,\n",
       "        2.88465769,  5.8786408 ,  3.01771838,  3.01128722,  3.90597129,\n",
       "        2.87774254,  3.94331187,  3.00057513,  4.54924502,  4.36052081,\n",
       "        5.38709004,  4.61300787,  4.54252325,  2.93028527,  4.58417194,\n",
       "        3.98731081,  4.93654787,  3.66869252,  3.69230717,  2.85268939,\n",
       "        4.59582409,  4.28189906,  4.56330038,  4.40912051,  4.6776022 ,\n",
       "        4.69610483,  4.11473512,  3.36005384,  4.22198046,  3.2044167 ,\n",
       "        2.80463566,  4.3199874 ,  4.1056914 ,  3.11338851,  4.95068594,\n",
       "        4.54689996,  4.21812587,  4.91002044,  2.99098854,  2.8410202 ,\n",
       "        2.52954115,  2.50157512,  2.54652696,  3.30666214,  3.83179154,\n",
       "        4.7384456 ,  4.4661152 ,  4.55704103,  4.23190417, 10.19212074,\n",
       "        4.051109  ,  4.27340052,  2.61481615,  2.00646306,  2.11292476,\n",
       "        2.19961986,  2.82414612,  4.31751894,  2.80893736,  4.56525952,\n",
       "        2.74718174,  4.20881187,  3.86524528,  2.64869665,  2.92340896,\n",
       "        2.20349323,  3.0234357 ,  2.10603439,  4.00741854,  5.08850911,\n",
       "        3.52494505,  4.32872318,  3.24561864,  4.67728141,  4.4834509 ,\n",
       "        4.15160202,  3.96980368,  3.14808134,  2.80492631,  3.23742772,\n",
       "        4.37450594,  4.38659133,  4.50012473, 10.43274961, 10.31678056,\n",
       "        4.48005172,  3.65737324,  3.0477679 ,  2.89959164,  4.04799971,\n",
       "        3.98604956,  4.94417003,  5.1587792 ,  3.02677914,  4.4809641 ,\n",
       "        4.57982536,  2.8272614 ,  4.53695414,  2.9979206 ,  4.33494885,\n",
       "        2.93265286,  2.61614006,  3.16453409,  4.15392276,  2.68681775,\n",
       "        2.76275598,  4.39384411,  6.58387901,  4.41289081,  4.24554025,\n",
       "        4.76718986,  4.50710994,  4.51604961, 10.31436242,  4.1898132 ,\n",
       "        4.18426855,  9.61645043,  9.29002043,  9.51176672,  9.33382919,\n",
       "        9.51489947,  4.33972264,  4.34080237,  9.71090524,  4.22190477,\n",
       "        4.06095373,  4.26379079,  3.69337521, 10.3796279 ,  4.37112518,\n",
       "        4.32752177,  4.48753136, 10.70344915, 10.65028693, 10.5541375 ,\n",
       "        4.16586742,  9.93216712,  4.15711742,  3.98818311,  4.75256725,\n",
       "       10.33197997,  4.32414624,  4.41920141,  4.15140464, 10.39206974,\n",
       "        4.46052087,  4.2415511 ,  3.67159587,  4.290445  ,  4.19435279,\n",
       "        3.14842451,  4.03459435,  4.22584122,  9.67637281,  4.21011316,\n",
       "        4.28060101, 10.42068326,  4.40950789,  4.15391451,  2.8708253 ,\n",
       "        2.7469336 ,  4.38934389,  3.84370313,  3.02145722,  2.80525885,\n",
       "        4.4087261 ,  2.98870306,  4.30437428, 10.85819678, 10.78234985,\n",
       "       10.75581786,  4.32882448,  3.10844451,  3.91497983,  2.62364413,\n",
       "        3.81755936,  4.18416205,  5.10431396,  2.96010661,  4.44884244,\n",
       "        4.22337897, 10.3115042 ,  4.36498151,  4.37135761,  2.62438164,\n",
       "        2.48627065,  4.32349888,  3.36072512,  3.62353427,  3.98999396,\n",
       "        2.94666619,  2.9722196 ,  4.31577199,  4.21880019,  4.58736519,\n",
       "        2.94210328,  3.74554857,  4.40220877,  2.87482215,  4.88923894,\n",
       "        4.1139168 ,  2.99908941,  4.09471959,  4.55593273,  4.35593261,\n",
       "        3.15935506,  5.30912887,  4.43304508,  3.13218373,  4.66283236,\n",
       "        4.59963416,  2.8293312 ,  4.25174177, 10.22654674,  4.4336496 ,\n",
       "        4.42744513,  4.48763943,  4.18133034,  4.20914252,  2.86465704,\n",
       "        2.77831912,  4.28319256,  4.96200832,  2.84414957,  4.32811338,\n",
       "        3.31962086,  4.02967926])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "27770943",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_f1(A, b, w, lambda_ridge=100):\n",
    "    return A.T @ A @ w - A.T @ b + lambda_ridge*w\n",
    "#     return A.T@(A@w-b)+lambda_ridge*w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "db91ef42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent_f1(A, b, w0, learning_step=0.0001, lambda_ridge=100, stop_condition=1, N_iter_max = 1000):\n",
    "    w = w0 # initialize the target\n",
    "    for i in range(N_iter_max):\n",
    "        gradient = gradient_f1(A,b,w,lambda_ridge)\n",
    "        if np.linalg.norm(gradient) < stop_condition:\n",
    "            break # leave the loop if we reach the stop condition\n",
    "        w = w - learning_step * gradient\n",
    "    return w, i+1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4381d20",
   "metadata": {},
   "source": [
    "Here in order to find the Lipschitz, we need to find the maximum value of $||\\nabla f_1||$. Involving finding the maximum singular values of $\\lambda I + A^TA$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79e02fb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2.863161135661648e-07+0j)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_ridge=100\n",
    "max_eigen = np.linalg.eigvals(A.T @ A).max() + lambda_ridge\n",
    "L = 1/(max_eigen)\n",
    "L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "52650fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([-0.01238055+0.j,  0.05775865+0.j, -0.00111773+0.j, ...,\n",
       "         0.01579911+0.j, -0.03571617+0.j,  0.01335282+0.j]),\n",
       " 54192)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "learning_step = 0.9 * 2 * L\n",
    "d = data_matrix_train.shape[1]\n",
    "w0 = np.zeros((2*d+1,))\n",
    "w_gradient_descent, N_iter_final = descent_f1(A, b, w0,learning_step=learning_step, N_iter_max = 100000)\n",
    "w_gradient_descent, N_iter_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "40b90580",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1785, 722)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# w_gradient_descent.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b95307",
   "metadata": {},
   "source": [
    "# Regularization for a sparse model\n",
    "\n",
    "## Question 4.1\n",
    "\n",
    "Here we can write $f_2 = \\min_w\\frac{1}{2}||Aw-b||^2$ and $g_2 = \\lambda ||w||_1$. Where $f_2$ is differentiable and $g_2$ is convex lower-semicontinous. The gardient of $f_2$ is $$\\nabla f_2(w) :\\; w\\mapsto A^T(Aw-b)$$\n",
    "\n",
    "The proximal operator of $g_2$ is $\\mathrm{prox}_g(x) = \\arg\\min_yg(y) + \\frac{1}{2}||x-y||^2$.\n",
    "\n",
    "$$\n",
    "\\mathrm{prox}_g(x) =\n",
    "\\begin{cases}\n",
    "x - \\lambda, & \\text{if } x > \\lambda  \\\\\n",
    "0, & \\text{if } |x| \\leq \\lambda  \\\\\n",
    "x + \\lambda, & \\text{if } x < -\\lambda\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "## Question 4.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "289703d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "expected ':' (84707397.py, line 6)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[55], line 6\u001b[1;36m\u001b[0m\n\u001b[1;33m    else if i < -lambda_lasso:\u001b[0m\n\u001b[1;37m         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m expected ':'\n"
     ]
    }
   ],
   "source": [
    "def prox_g(x, lambda_lasso = 200):\n",
    "    res = []\n",
    "    for i in x:\n",
    "        if i > lambda_lasso:\n",
    "            res.append(i-lambda_lasso)\n",
    "        else if i < -lambda_lasso:\n",
    "            res.append(i+lambda_lasso)\n",
    "        else:\n",
    "            res.append(0)\n",
    "    return np.array(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27bfe518",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_f2(A, b, w):\n",
    "    return A.T@(A@w-b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "356e45ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def descent_prox_lasso(A, b, w0, learning_step, lambda_lasso = 200, stop_condition=1, N_iter_max = 1000):\n",
    "    w = w0 # initialize the target\n",
    "    for i in range(N_iter_max):\n",
    "        gradient = gradient_f2(A,b,w,lambda_ridge)\n",
    "        if np.linalg.norm(gradient) < stop_condition:\n",
    "            break # leave the loop if we reach the stop condition\n",
    "        w = prox_g(w - learning_step * gradient_f2(A,b,w), lambda_lasso = lambda_lasso * learning_step)\n",
    "    return w, i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06291ba6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a3612a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c2142fe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ima-env",
   "language": "python",
   "name": "ima-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
